{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mixed GRU and metadata NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNfvTdGzK4+vxB1UWn6hGja",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrcyme/Mixed-GRU-and-metadata-NLP./blob/main/Mixed_GRU_and_metadata_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGA41DDCDx9j"
      },
      "source": [
        "# Mixed gru and metadata model\n",
        "\n",
        "It was asked to predict the destination of a new question based on a dataset of published parliamentary questions scraped from the chd.lu site.\n",
        "\n",
        "My approach is to use a combination of a GRU network using the subject of the questions as input and simple a DNN using some metadata as input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4dG5SsB-ZjO",
        "outputId": "8e2861b7-0e0d-44e2-ae36-9cba927826c4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import MultiLabelBinarizer,LabelEncoder\n",
        "import re\n",
        "import nltk\n",
        "from tensorflow.keras.layers import Dense, Embedding, GRU, Input, Concatenate\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from google.colab import drive \n",
        "drive.mount(\"/content/gdrive\") \n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofOyjz_ev7VU"
      },
      "source": [
        "### Model Hyper parameters\n",
        "The model hyperparameters have been fixed based on literature and my personal experience. To find better combination, formal hyperpareters optimisation could be run.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxndJXCdsAE5"
      },
      "source": [
        "MAX_WORDS = 80000\n",
        "EMB_DIM = 100\n",
        "BATCH_SIZE = 64\n",
        "N_EPOCHS = 20\n",
        "VECTOR_LEN =  62\n",
        "DROUPOUT_RATE = 0.6\n",
        "HIDDEN_DIM = 100\n",
        "LEARNING_RATE = 0.002\n",
        "EARLY_STOPPING = EarlyStopping(monitor='val_loss', \n",
        "                               mode='min',\n",
        "                               restore_best_weights=True,\n",
        "                               patience = 2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S85v7Pn3wDiQ"
      },
      "source": [
        "## Data loading and preprocessing\n",
        "\n",
        "In the following cell, the data are loaded and preprocessed. The destination columns is refactored in order to keep only the ministries (the name of the minister is irrelevant in the given situation). None values in the ministries section are removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl2rA8MX-eUZ"
      },
      "source": [
        "question_df = pd.read_json(\"https://download.data.public.lu/resources/parliamentary-questions/20210220-115703/questions.json\", orient=\"index\")\n",
        "question_df = question_df.drop(axis=1,columns=['answer_by','answer_date','answer_limit_date','date','qp_number','url','has_answer','answer_type'])\n",
        "question_df.destinations = [list(filter(None,[d[\"ministry\"] for d in q])) for q in question_df.destinations]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csKz7SnrZ81Y"
      },
      "source": [
        "  I choose to remove questions that are asked to ministries that received less than 50 questions. Indeed, in my opinion, I don't have enough data to be able to classify them correctly. As the final prediction should be among the 10% top ministeries, I could keep only the questions asked to them. However, as it represents only 65% of the questions, it would be a bit peculiar to generate a model that is relevant in only 65% of the cases.\n",
        "\n",
        "\n",
        "  As questions can be presented to more than one ministery, we have a multilabel classification problem.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE7pET4lGx_a",
        "outputId": "0853ca5b-7d02-47a9-c40a-71f4af0d5af8"
      },
      "source": [
        "#some insight\n",
        "flat_list = [m for q in question_df.destinations for m in q]\n",
        "count = Counter(flat_list)\n",
        "top_ministries = [count.most_common()[i][0] for i in range(int(np.ceil(0.1*len(count))))]\n",
        "under_50_questions = [k for k, v in count.items() if v<50]\n",
        "output_dim = len(count)-len(under_50_questions)\n",
        "print(\"The top 10% ministries receiving the most questions are : \\n{}\".format(\", \".join(top_ministries)))\n",
        "print(\"\")\n",
        "print(\"The ministeeries that have received less than 50 questions are : \\n{}\".format(\", \".join(under_50_questions)))\n",
        "print(\"\")\n",
        "print(\"Each question is asked to {} ministeries on average\".format(np.mean(question_df.destinations.apply(lambda x: len(x)))))\n",
        "print(\"\")\n",
        "print(\"Each question is asked by {} authors on average\".format(np.mean(question_df.authors.apply(lambda x: len(x)))))\n",
        "print(\"\")\n",
        "print(\"The top 10% ministries are targetted by {}% of the questions\".format(np.sum([count[m] for m in top_ministries])/len(question_df)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The top 10% ministries receiving the most questions are : \n",
            "Ministre de la Santé, Ministre du Développement durable et des Infrastructures, Ministre de l'Environnement, Ministre de la Justice, Ministre des Finances, Ministre de l'Education nationale, Ministre de la Sécurité sociale, Ministre de l'Intérieur, Ministre des Transports, Premier Ministre\n",
            "\n",
            "The ministeeries that have received less than 50 questions are : \n",
            "Ministre de la Promotion féminine, Ministre de la Jeunesse, Ministre de la Force publique, Secrétaire d'Etat aux Travaux publics, Ministre de l'Aménagement du Territoire, Secrétaire d'Etat aux Affaires étrangères, Ministre de l'Education physique et des Sports, Ministre du Budget, Ministre aux Handicapés et aux Accidentés de la Vie, Secrétaire d'Etat à l'Environnement, Secrétaire d'Etat de la Fonction publique et de la Réforme administrative, Ministre aux Relations avec le Parlement, Ministre par interim de la Culture, Ministre par interim des Travaux publics, Ministre de l'Egalité des chances, Ministre délégué aux Affaires étrangères et à l'Immigration, Vice-Premier Ministre, Secrétaire d'Etat à l'Agriculture, Secrétaire d'Etat aux Relations avec le Parlement, Ministre du Trésor, Ministre délégué à l'Economie solidaire, Ministre déléguée à la Fonction publique et à la Réforme administrative, Ministre à la Simplification administrative auprès du Premier Ministre, Ministre à la Grande Région, Secrétaire d'Etat à l'Education nationale, Secrétaire d'Etat à l'Enseignement supérieur et à la Recherche, Secrétaire d'Etat à la Défense, Ministre de la Coopération et de l'Action humaine, Secrétaire d'Etat à l'Economie, Secrétaire d'Etat au Développement durable et aux Infrastructures, Ministre délégué à l'Enseignement supérieur et à la Recherche, Secrétaire d'Etat à la Sécurité intérieure, Ministre de la Digitalisation, Ministre de la Grande Région, Ministre de l'Égalité entre les femmes et les hommes, Ministre de la Réforme administrative, Ministre du Tourisme, Ministre délégué à la Digitalisation, Ministre délégué à la Réforme administrative, Ministre d, Ministre délégué à la Sécurite intérieure, Ministre délégué à la Défense, Ministre délégué à la Sécurité intérieure, Ministre déléguée de la Sécurité sociale\n",
            "\n",
            "Each question is asked to 1.2953287800987088 ministeries on average\n",
            "\n",
            "Each question is asked by 1.1216411585821826 authors on average\n",
            "\n",
            "The top 10% ministries are targetted by 0.6565132858068697% of the questions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX-qqg1i17d3"
      },
      "source": [
        "#Remove question asked to ministeries that received less than 50 questions\n",
        "question_df.destinations = question_df.destinations.apply(lambda x: list(filter(lambda y: y not in under_50_questions, x)))\n",
        "question_df = question_df[question_df.destinations.str.len()>0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrDxgEUfQdIm"
      },
      "source": [
        "## Model generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq2ahVS1bA5R"
      },
      "source": [
        "The following cell contains all the functions used in the pipeline. Note I use glove embedding to serve as input of the GRU. The glove pretrained vector must then be uploaded to this notebook (here via drive).\n",
        "\n",
        "Three models are generated. The first one uses only the subject of the question as input. Text are processed and fed into a GRU model. The second model uses only the metadata available. The chosen metadata are the authors of the question and the type of question. Indeed, all the data about the answer could be used, but then it does not make any sense to predict the destination if information about the answer are available. The last model is a combination of the two previous models.\n",
        "\n",
        "As it is asked to predict only one destination, the metric used to characterise the quality of the model is the average precision at 1. In other words, a prediction is considered as sucessful if the destination predicted is among the true destinations.\n",
        "Other metric could be used such as average mean precision score at k if more than one label are predicted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyOpbmytZ95g"
      },
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Clean text.\"\"\"\n",
        "    text = re.sub(\"\\'\", \"\", text)\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    text = text.lower()\n",
        "    stopWords = set(stopwords.words('english'))\n",
        "    text = ' '.join([w for w in text.split() if w not in stopWords])\n",
        "    return text\n",
        "\n",
        "def get_embedded_subject(df,tokenizer=None):\n",
        "    \"\"\"Preprocess synopsis.\"\"\"\n",
        "    x_train = df.subject.apply(lambda x: clean_text(x)).to_numpy()\n",
        "    if not tokenizer:\n",
        "      tokenizer = text.Tokenizer(num_words=MAX_WORDS)\n",
        "      tokenizer.fit_on_texts(x_train)\n",
        "    x_train_seq = tokenizer.texts_to_sequences(x_train)\n",
        "    x_train_pad = sequence.pad_sequences(x_train_seq, maxlen=VECTOR_LEN)\n",
        "    return x_train_pad, tokenizer\n",
        "\n",
        "def generate_glove_embedding(tokenizer):\n",
        "    \"\"\"Generate glove embedding matrix.\n",
        "\n",
        "    Uses the glove's pretrained vectors to generate an embedding matric\n",
        "\n",
        "    :param tokenizer : tokenizer used to convert synopsis to int sequences\n",
        "    :return : The embedding matrix used in the embedding layer of the GRU\n",
        "    \"\"\"\n",
        "    embeddings_dictionary = dict()\n",
        "    with open('/content/gdrive/MyDrive/glove.6B.100d.txt', encoding=\"utf8\") as glove_file:\n",
        "        for line in glove_file:\n",
        "            records = line.split()\n",
        "            vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
        "            embeddings_dictionary[records[0]] = vector_dimensions\n",
        "    embedding_matrix = np.zeros((MAX_WORDS, 100))\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        embedding_vector = embeddings_dictionary.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def preprocess_destinations(df):\n",
        "    \"\"\"Convert genres to multinomial one hot representation.\n",
        "\n",
        "    A list of genre is converted to a vector of length equal\n",
        "    to the total number of genre.\n",
        "    The entry corresponding to a certain genre equals\n",
        "    one divided by the number of genre in the list if the genre is present\n",
        "    and zero otherwise.\n",
        "\n",
        "    :param df : Dataframe containing a genres column\n",
        "    :return : one hot encodding of the genres\n",
        "    \"\"\"\n",
        "    destination_one_hot = MLB.fit_transform(df.destinations)\n",
        "    destination_one_hot = np.array([e/sum(e) for e in destination_one_hot])\n",
        "    return destination_one_hot\n",
        "\n",
        "def generate_gru(output_dim,tokenizer):\n",
        "    \"\"\"Generate GRU model.\n",
        "\n",
        "    Generate the GRU model with the defined config.\n",
        "\n",
        "    :param : tokenizer : tokenizer used to convert text to integer sequence\n",
        "    :return : compiled GRU model\n",
        "    \"\"\"\n",
        "    embedding_matrix = generate_glove_embedding(tokenizer)\n",
        "    model_gru = Sequential()\n",
        "    model_gru.add(Embedding(MAX_WORDS,\n",
        "                            EMB_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=True))\n",
        "    model_gru.add(GRU(HIDDEN_DIM,\n",
        "                  dropout=DROUPOUT_RATE,\n",
        "                  return_sequences=False))\n",
        "    model_gru.add(Dense(output_dim, activation='softmax'))\n",
        "    optimizer = Adam(learning_rate=LEARNING_RATE)\n",
        "    model_gru.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=optimizer,\n",
        "                      metrics=['categorical_accuracy'])\n",
        "    return model_gru\n",
        "\n",
        "\n",
        "def generate_meta_data_model():\n",
        "    model_meta = Sequential()\n",
        "    model_meta.add(Input(shape=(182,)))\n",
        "    model_meta.add(Dense(100, activation='relu'))\n",
        "    model_meta.add(Dense(100, activation='relu'))\n",
        "    model_meta.add(Dense(output_dim, activation='softmax'))\n",
        "    model_meta.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
        "    return model_meta\n",
        "\n",
        "def generate_mixed_model(tokenizer):\n",
        "\n",
        "    embedding_matrix = generate_glove_embedding(tokenizer)\n",
        "    model_gru = Sequential()\n",
        "    model_gru.add(Embedding(MAX_WORDS,\n",
        "                            EMB_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=True))\n",
        "    model_gru.add(GRU(HIDDEN_DIM,\n",
        "                  dropout=DROUPOUT_RATE,\n",
        "                  return_sequences=False))\n",
        "    \n",
        "    model_meta = Sequential()\n",
        "    model_meta.add(Input(shape=(182,)))\n",
        "    model_meta.add(Dense(100, activation='relu'))\n",
        "    model_meta.add(Dense(100, activation='relu'))\n",
        "    model_meta.add(Dense(output_dim, activation='softmax'))\n",
        "    concat = Concatenate()([model_gru.output,model_meta.output])\n",
        "    concat = Dense(output_dim, activation='softmax')(concat)\n",
        "    model_mixed = Model(inputs=[model_gru.input, model_meta.input], outputs=concat)\n",
        "    model_mixed.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
        "    return model_mixed\n",
        "\n",
        "def custom_precision(predicted_top, actual):\n",
        "  p = 0\n",
        "  for i in range (len(actual)):\n",
        "    if predicted_top[i][0] in actual[i]:\n",
        "      p+=1\n",
        "  return p/len(actual)\n",
        "\n",
        "def train(x_train, y_train, model):\n",
        "  model.fit(x_train, y_train, batch_size=BATCH_SIZE, validation_split=0.1, epochs=N_EPOCHS,callbacks=[EARLY_STOPPING])\n",
        "  return model\n",
        "\n",
        "def test(x_test, y_test, model):\n",
        "  y_pred_one_hot = model.predict(x_test)\n",
        "  y_pred_top_one_hot = np.array([e==e.max() for e in y_pred_one_hot])\n",
        "  y_pred_top = MLB.inverse_transform(y_pred_top_one_hot)\n",
        "  return custom_precision(y_pred_top, y_test)\n",
        "\n",
        "def predict_most_probable_top_ten_destination(x, top_ten, model):\n",
        "  y_pred_one_hot = model.predict(x)\n",
        "  mask = [e in top_ten for e in MLB.classes_]\n",
        "  y_pred_top_one_hot = np.array([e==e.max() for e in y_pred_one_hot*mask])\n",
        "  return MLB.inverse_transform(y_pred_top_one_hot)\n",
        "  \n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBKuZCThJqDs"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHCMuwC5r655"
      },
      "source": [
        "MLB = MultiLabelBinarizer()\n",
        "LE = LabelEncoder()\n",
        "MLB_authors = MultiLabelBinarizer()\n",
        "meta_vector = np.hstack((MLB_authors.fit_transform(question_df.authors),to_categorical(LE.fit_transform(question_df.qp_type))))\n",
        "msk = np.random.rand(len(question_df)) < 0.8\n",
        "train_df = question_df[msk]\n",
        "test_df = question_df[~msk]\n",
        "x_train_meta = meta_vector[msk]\n",
        "x_test_meta = meta_vector[~msk]\n",
        "x_train, tokenizer = get_embedded_subject(train_df)\n",
        "y_train = preprocess_destinations(train_df)\n",
        "x_test, _ = get_embedded_subject(test_df, tokenizer)\n",
        "y_test = test_df.destinations.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3cZsTLcqRxg"
      },
      "source": [
        "### Gru model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhC8clu34gmO",
        "outputId": "bf78c23a-91c3-4352-a67e-ac1d803f41f1"
      },
      "source": [
        "model_gru = generate_gru(output_dim, tokenizer)\n",
        "model_gru_fitted = train(x_train, y_train, model_gru)\n",
        "test(x_test, y_test, model_gru_fitted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "222/222 [==============================] - 52s 85ms/step - loss: 3.5891 - categorical_accuracy: 0.0878 - val_loss: 3.4970 - val_categorical_accuracy: 0.1598\n",
            "Epoch 2/20\n",
            "222/222 [==============================] - 18s 82ms/step - loss: 3.0490 - categorical_accuracy: 0.2134 - val_loss: 3.0011 - val_categorical_accuracy: 0.2809\n",
            "Epoch 3/20\n",
            "222/222 [==============================] - 18s 82ms/step - loss: 2.5360 - categorical_accuracy: 0.3299 - val_loss: 2.6767 - val_categorical_accuracy: 0.3291\n",
            "Epoch 4/20\n",
            "222/222 [==============================] - 18s 82ms/step - loss: 2.1979 - categorical_accuracy: 0.4035 - val_loss: 2.6435 - val_categorical_accuracy: 0.3551\n",
            "Epoch 5/20\n",
            "222/222 [==============================] - 18s 82ms/step - loss: 1.9572 - categorical_accuracy: 0.4570 - val_loss: 2.6525 - val_categorical_accuracy: 0.3691\n",
            "Epoch 6/20\n",
            "222/222 [==============================] - 18s 81ms/step - loss: 1.7861 - categorical_accuracy: 0.4957 - val_loss: 2.6319 - val_categorical_accuracy: 0.3678\n",
            "Epoch 7/20\n",
            "222/222 [==============================] - 18s 81ms/step - loss: 1.6333 - categorical_accuracy: 0.5387 - val_loss: 2.6723 - val_categorical_accuracy: 0.3798\n",
            "Epoch 8/20\n",
            "222/222 [==============================] - 18s 80ms/step - loss: 1.5340 - categorical_accuracy: 0.5620 - val_loss: 2.6737 - val_categorical_accuracy: 0.3767\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.48257839721254353"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs66PYBrhww-"
      },
      "source": [
        "## Meta Data model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSwB92iRh2Bs",
        "outputId": "d2165758-2a03-47f7-be42-7a207cb659b1"
      },
      "source": [
        "model_meta = generate_meta_data_model()\n",
        "model_meta_fitted = train(x_train_meta, y_train, model_meta)\n",
        "test(x_test_meta, y_test, model_meta_fitted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 3.6474 - categorical_accuracy: 0.0782 - val_loss: 3.6084 - val_categorical_accuracy: 0.0913\n",
            "Epoch 2/20\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 3.1737 - categorical_accuracy: 0.1903 - val_loss: 3.4213 - val_categorical_accuracy: 0.1141\n",
            "Epoch 3/20\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 2.9974 - categorical_accuracy: 0.2031 - val_loss: 3.3020 - val_categorical_accuracy: 0.1370\n",
            "Epoch 4/20\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 2.9177 - categorical_accuracy: 0.2150 - val_loss: 3.2700 - val_categorical_accuracy: 0.1465\n",
            "Epoch 5/20\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 2.8705 - categorical_accuracy: 0.2238 - val_loss: 3.2683 - val_categorical_accuracy: 0.1357\n",
            "Epoch 6/20\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 2.8383 - categorical_accuracy: 0.2265 - val_loss: 3.2374 - val_categorical_accuracy: 0.1547\n",
            "Epoch 7/20\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 2.8256 - categorical_accuracy: 0.2247 - val_loss: 3.2168 - val_categorical_accuracy: 0.1509\n",
            "Epoch 8/20\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 2.8038 - categorical_accuracy: 0.2258 - val_loss: 3.1921 - val_categorical_accuracy: 0.1465\n",
            "Epoch 9/20\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 2.7837 - categorical_accuracy: 0.2274 - val_loss: 3.1868 - val_categorical_accuracy: 0.1370\n",
            "Epoch 10/20\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 2.7628 - categorical_accuracy: 0.2298 - val_loss: 3.2156 - val_categorical_accuracy: 0.1395\n",
            "Epoch 11/20\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 2.7583 - categorical_accuracy: 0.2304 - val_loss: 3.2517 - val_categorical_accuracy: 0.1465\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.24265803882528622"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_HxKw_0J1zT"
      },
      "source": [
        "## Mixed Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x5hAhZBhwKm",
        "outputId": "396a7674-08e3-423e-c99f-4c53a179bf67"
      },
      "source": [
        "model_mixed = generate_mixed_model(tokenizer)\n",
        "model_mixed = train([x_train,x_train_meta], y_train, model_mixed)\n",
        "test([x_test,x_test_meta], y_test, model_mixed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "222/222 [==============================] - 20s 84ms/step - loss: 3.6401 - categorical_accuracy: 0.0730 - val_loss: 3.5725 - val_categorical_accuracy: 0.1306\n",
            "Epoch 2/20\n",
            "222/222 [==============================] - 18s 83ms/step - loss: 3.2679 - categorical_accuracy: 0.1643 - val_loss: 3.3444 - val_categorical_accuracy: 0.1833\n",
            "Epoch 3/20\n",
            "222/222 [==============================] - 18s 82ms/step - loss: 2.9198 - categorical_accuracy: 0.2484 - val_loss: 3.0149 - val_categorical_accuracy: 0.2606\n",
            "Epoch 4/20\n",
            "222/222 [==============================] - 18s 82ms/step - loss: 2.6218 - categorical_accuracy: 0.3123 - val_loss: 2.8241 - val_categorical_accuracy: 0.3196\n",
            "Epoch 5/20\n",
            "222/222 [==============================] - 18s 83ms/step - loss: 2.3564 - categorical_accuracy: 0.3836 - val_loss: 2.6827 - val_categorical_accuracy: 0.3291\n",
            "Epoch 6/20\n",
            "222/222 [==============================] - 18s 82ms/step - loss: 2.1693 - categorical_accuracy: 0.4203 - val_loss: 2.6068 - val_categorical_accuracy: 0.3538\n",
            "Epoch 7/20\n",
            "222/222 [==============================] - 18s 82ms/step - loss: 1.9940 - categorical_accuracy: 0.4660 - val_loss: 2.5465 - val_categorical_accuracy: 0.3710\n",
            "Epoch 8/20\n",
            "222/222 [==============================] - 18s 82ms/step - loss: 1.8667 - categorical_accuracy: 0.5014 - val_loss: 2.5166 - val_categorical_accuracy: 0.3786\n",
            "Epoch 9/20\n",
            "222/222 [==============================] - 18s 80ms/step - loss: 1.7470 - categorical_accuracy: 0.5184 - val_loss: 2.4673 - val_categorical_accuracy: 0.3893\n",
            "Epoch 10/20\n",
            "222/222 [==============================] - 18s 81ms/step - loss: 1.6535 - categorical_accuracy: 0.5463 - val_loss: 2.4489 - val_categorical_accuracy: 0.3855\n",
            "Epoch 11/20\n",
            "222/222 [==============================] - 18s 80ms/step - loss: 1.5686 - categorical_accuracy: 0.5646 - val_loss: 2.4717 - val_categorical_accuracy: 0.3938\n",
            "Epoch 12/20\n",
            "222/222 [==============================] - 18s 81ms/step - loss: 1.5016 - categorical_accuracy: 0.5838 - val_loss: 2.4262 - val_categorical_accuracy: 0.3957\n",
            "Epoch 13/20\n",
            "222/222 [==============================] - 18s 81ms/step - loss: 1.4352 - categorical_accuracy: 0.6086 - val_loss: 2.5059 - val_categorical_accuracy: 0.3887\n",
            "Epoch 14/20\n",
            "222/222 [==============================] - 18s 80ms/step - loss: 1.3435 - categorical_accuracy: 0.6276 - val_loss: 2.4584 - val_categorical_accuracy: 0.3919\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5343454454952713"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQbRUFD1uRdS"
      },
      "source": [
        "We see that the best performing model is the mixed model that benefits both from the metadata and the subjects. 53% could appear as a bad score. However, this classification problem is quite difficult. Indeed, there is a high number of possible destinations and subjects are very short and sometimes do not provide enough information to classify well. Furthermore, the fact that the model only outputs one destination is a strong constraint. A better use of the model would be, for example, to predict the five top destinations and the to calculate the average mean precision score at 5.\n",
        "\n",
        "Experience with keeping only the questions asked to the 10% top ministries have shown a higher score (74%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PuffZZ-KP0T"
      },
      "source": [
        "## Final model \n",
        "Above is the final model trained on the hole dataset. To generate the most probable destination amongst the top ten, the function predict_most_probable_top_ten_destination can be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNv3D-6IN0LS",
        "outputId": "04cd7a63-993b-4ad1-92d2-8101e9685bc7"
      },
      "source": [
        "x_train, tokenizer = get_embedded_subject(question_df)\n",
        "y_train = preprocess_destinations(question_df)\n",
        "model_final = generate_mixed_model(tokenizer)\n",
        "model_final = train([x_train,meta_vector], y_train, model_final)\n",
        "#to generate prediction from new data : \n",
        "#predictions = predict_most_probable_top_ten_destination([x_predict,x_predict_meta], top_ministries, model_final)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "279/279 [==============================] - 24s 81ms/step - loss: 3.6037 - categorical_accuracy: 0.0855 - val_loss: 3.5220 - val_categorical_accuracy: 0.1537\n",
            "Epoch 2/20\n",
            "279/279 [==============================] - 22s 79ms/step - loss: 3.1782 - categorical_accuracy: 0.1895 - val_loss: 3.1151 - val_categorical_accuracy: 0.2432\n",
            "Epoch 3/20\n",
            "279/279 [==============================] - 22s 80ms/step - loss: 2.7597 - categorical_accuracy: 0.2916 - val_loss: 2.8739 - val_categorical_accuracy: 0.2907\n",
            "Epoch 4/20\n",
            "279/279 [==============================] - 22s 80ms/step - loss: 2.4323 - categorical_accuracy: 0.3610 - val_loss: 2.7124 - val_categorical_accuracy: 0.3281\n",
            "Epoch 5/20\n",
            "279/279 [==============================] - 22s 77ms/step - loss: 2.2012 - categorical_accuracy: 0.4217 - val_loss: 2.5323 - val_categorical_accuracy: 0.3680\n",
            "Epoch 6/20\n",
            "279/279 [==============================] - 22s 80ms/step - loss: 2.0409 - categorical_accuracy: 0.4562 - val_loss: 2.5001 - val_categorical_accuracy: 0.3696\n",
            "Epoch 7/20\n",
            "279/279 [==============================] - 22s 77ms/step - loss: 1.9012 - categorical_accuracy: 0.4899 - val_loss: 2.3977 - val_categorical_accuracy: 0.3852\n",
            "Epoch 8/20\n",
            "279/279 [==============================] - 22s 77ms/step - loss: 1.7653 - categorical_accuracy: 0.5192 - val_loss: 2.3922 - val_categorical_accuracy: 0.3888\n",
            "Epoch 9/20\n",
            "279/279 [==============================] - 21s 77ms/step - loss: 1.6707 - categorical_accuracy: 0.5370 - val_loss: 2.3522 - val_categorical_accuracy: 0.4044\n",
            "Epoch 10/20\n",
            "279/279 [==============================] - 22s 78ms/step - loss: 1.5726 - categorical_accuracy: 0.5613 - val_loss: 2.3786 - val_categorical_accuracy: 0.4055\n",
            "Epoch 11/20\n",
            "279/279 [==============================] - 22s 77ms/step - loss: 1.4859 - categorical_accuracy: 0.5818 - val_loss: 2.3848 - val_categorical_accuracy: 0.4060\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzYT6Ye7wq5B"
      },
      "source": [
        "# Additional questions : \n",
        "4. It is difficult to list the drawbacks of my prototype since I don't see a real use case for the model. Indeed the only use case I see would be to provide a suggestion list in the destination section of a parliamentarian mailbox. In that case, oral question should surely be removed from the dataset. Furthermore, I cannot see any reason to predict destination only in the top 10%.\n",
        "On the algorithmic point of view, the model is not perfectly optimized. Hyperparameter optimisation should be run and other model should be compared as well. \n",
        "\n",
        "5. There is a lot of thinking lacking for the solution to become part of a production environment. Amongst them: a proper data acquisition and preprossing pipeline, allowing to fetch new data on a daily base, an online learning scheme allowing to retrain the model periodically with the new data acquired,...\n",
        "\n",
        "6. If I think in term of the use case described above, there are some things to I would do to get better performance. First, I would delete all oral questions from the dataset (removing the attribute qp-type from the metadata in the same time). I would then retrain the model and allow him to predict a fixed number of destinations (to provide more than one suggestion to the user). I would find better hyperparameters combinations as well. In the context of a user mailbox, I would investigate some ways to use the historical data of the user to improve the prediction quality.\n",
        "\n",
        "7. I took 6h to write the code and 1h to comment it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBTD58HfN7K3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}